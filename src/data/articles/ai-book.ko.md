---
title: '"기계는 왜 학습하는가 : AI를 움직이는 우아한 수학" 을 읽고 1장'
description: '기계학습의 출발점인 선형 모델과 지도학습 개념을 통해, 데이터에서 패턴을 찾는 원리를 이해합니다.
퍼셉트론과 헤브 학습 이론을 통해 인공 신경망의 시작과 학습의 본질을 살펴봅니다.'
thumbnail: '/images/article_ai1.png'
date: '2025-05-27'
tags: ['AI', '독후감']
readTime: 30
featured: true
---

# 1장 패턴을 찾고 말 테다

### 1️⃣ “선형 방정식의 계수, 또는 가중치”란?

> 우리는 한편에 있는 y와 다른 편에 있는 x1과 x2가 이른바 선형관계를 이룬다는 사실을 알게 되었다. 선형이란 y가 x1, x2에 비례해서 달라질 뿐, x1이나 x2의 거듭제곱수나 x1 x2의 곱에 비례해 달라지지 않는다는 뜻이다. -p18

```
y = w1 * x1 + w2 * x2
```

이 식은 **선형 방정식(linear equation)**입니다.

y: 예측하고 싶은 값 (결과)
x1, x2: 입력값 (원인, 특성, feature)
w1, w2: 각각의 입력에 곱해지는 숫자 (이걸 계수 또는 가중치라고 부름)

계수(coefficient) : 수학적인 표현에서, 변수 앞에 곱해지는 숫자를 말해요. w1은 x1의 계수입니다.
가중치(weight) : 머신러닝에서 흔히 쓰이는 용어인데, 각 입력값의 중요도를 나타내요.
예를 들어 w1 = 10, w2 = 0.1이면 x1이 결과값에 훨씬 더 큰 영향을 준다는 뜻이에요.

→ 즉, 가중치 = 변수에 곱해지는 숫자 = 결과에 미치는 영향의 크기를 나타내는 수치

> #### ※ 참고 : 선형(linear)이란?
>
> 수학에서 선형 관계란 다음처럼 단순한 형태를 말해요

```
y = w1 * x1 + w2 * x2 + b
```

x1이나 x2에 곱해지는 건 숫자(w1, w2)뿐. x1이나 x2가 제곱(x1²) 되거나 곱해지지(x1 \* x2) 않아요.
쉽게 말하면 “선형”이란, 결과 y가 입력값 x1, x2에 단순히 ‘곱해서 더한 것’으로만 결정되는 관계를 말해요. 즉, 곱하거나 제곱하거나, 복잡한 함수 형태가 들어가면 선형이 아닙니다.
책에서 말하고자 하는 바는 선형 모델은 단순해서 빠르고 해석이 쉬워요.
하지만 복잡한 문제는 선형 모델로 해결이 어려워서, **비선형 모델(신경망 등)**을 쓰게 돼요.
![선형 비선형 구별식](https://velog.velcdn.com/images/imeureka/post/7a2951c7-188a-4fbd-a45f-366fb671c1bc/image.png)

![가중치](https://velog.velcdn.com/images/imeureka/post/1afe08f1-5b79-4fd4-acb3-1d7791ba92aa/image.png)

### 2️⃣ 학습이란,

“여기서 학습이 빛을 발한다…” 이게 무슨 말일까?
이 부분을 쉽게 풀면 이렇게 말하는 거예요

> 만약에 w1과 w2를 미리 알고 있다면, 어떤 x1, x2가 들어와도 y를 계산할 수 있지?
> 그런데 현실에서는 보통 w1, w2를 모르잖아. 이걸 사람이 일일이 계산하기는 어렵고.

그래서 기계가(알고리즘이) 이런 w1, w2를 스스로 찾아내도록 학습시킨다는 거예요.
데이터를 보고 컴퓨터가 **“어떤 w1, w2를 쓰면 저 y값들을 잘 예측할 수 있을까?”**를 고민하면서,
수많은 시도 끝에 가장 적절한 w1, w2를 찾아가는 과정 = 학습이라고 하는 거예요.

💼 정리하면
**가중치(w1, w2)**는 각 입력값(x1, x2)이 결과(y)에 미치는 영향력.

계수 = 가중치, 수학에선 계수라고 부르고 머신러닝에선 가중치라고도 불러요.

우리가 원하는 건 y = w1x1 + w2x2 같은 모델을 만드는 것.

하지만 w1, w2는 모름 → **데이터로부터 그 값을 찾아내는 것이 머신러닝의 핵심.**

이 찾는 과정을 **학습(learning)**이라고 부르는 거고, 그래서 “여기서 학습이 빛을 발한다”는 거예요.

### 3️⃣ 이것이 현실과 무슨 상관일까?

> x1이 주택의 침실 개수이고 x2가 전체 면적 y가 주택 가격이라 하면 (x1, x2) 와 y 사이에 선형 관계가 존재한다고 가정하자. 이제 주택과 가격의 기본 데이터로부터 선형방정식의 가중치를 학습하면, 침실 개수와 면적이 주어졌을 때 주택 가격을 예측하는 매우 간단한 모형을 만들 수 있다. 위의 예제는 하찮고 시시하지만 기계 학습의 출발점이다. 우리가 해낸 일은 단순화된 형태의 **지도학습 supervised learning** 이다. - p20

**지도학습(supervised learning)**이란 **“정답을 보고 배우는 학습”**이에요.

| 침실 수(x1) | 면적(x2) | 집값(y, 라벨) |
| ----------- | -------- | ------------- |
| 2           | 50㎡     | 2000만 원     |
| 3           | 70㎡     | 2800만 원     |
| 4           | 90㎡     | 3600만 원     |

이 표에는 **입력(x1, x2)**와 **정답(y)**이 함께 있어요.
즉, **"이런 입력에는 이런 정답이 있다"**는 식으로 **지도(가이드)**를 받은 상태

그래서 이건 마치 선생님이 옆에서 답을 알려주면서 학습을 시켜주는 방식이에요.

👉 그래서 "지도(supervised)"라는 말을 붙여요.

입력 집합: x1 = 침실 수, x2 = 면적
출력 집합: y = 집값

표처럼 생긴 데이터에서는 x1, x2와 y 사이에 패턴, 즉 규칙이나 관계가 숨어 있어요.
그런데 이 관계는 겉으로 보면 바로 드러나지 않아요.
그래서 학습을 통해 이 관계(패턴)를 발견하는 과정이 필요한 거예요.

> “독립변수 (x1, x2)가 주어졌을 때, 종속변수 (y)의 값을 예측하는 모형(또는 방정식)” → 이것이 회귀(regression)

**회귀(Regression)**는
어떤 **입력 값(x)**이 주어졌을 때, 그에 따른 **연속적인 출력 값(y)**을 예측하는 것이에요.
즉, 앞서 다룬 문단들은 전부 **회귀(regression)**라는 기계학습의 한 가지 방법을 설명하기 위한 흐름이에요.

> **※ 참고 : 왜 x1, x2가 독립변수라고 하나요?**
> 독립변수(x): 우리가 조절하거나 입력하는 값이에요.
> 종속변수(y): 입력값에 의해 결정되는 출력값, 즉 예측하고 싶은 값이에요.
> 예를 들어
> 침실 수(x1), 면적(x2)은 우리가 입력으로 주는 독립적인 변수
> 집값(y)은 그 값들에 의존해서 나오는 값 → 종속 변수
> 즉, 우리가 조작 가능한 입력 값이 독립변수,결과로 나오는 출력 값이 종속변수예요.

### 4️⃣ “새끼 오리의 학습은 양태가 더 정교해보인다...”

제가 인상적으로 읽었던 문구였어요. 이건 인간과 동물은 지도학습 없이도 학습할 수 있다는 흥미로운 지적이에요.

앞에서 우리가 한 건 지도학습이었죠?
x와 y가 함께 주어져 있어요 (데이터에 정답이 있어요)

**그런데 새끼 오리는 어떻죠?**

태어나자마자 주변에서 엄마 오리를 따라다니며 스스로 학습해요
정답(라벨)이 주어진 게 아니라 그냥 환경을 보고 학습하는 거예요

이건 사실상 비지도학습, 또는 강화학습 같은 더 복잡한 형태예요, 그런데도 생명체는 이걸 자연스럽게 잘 해냅니다.

> “새끼 오리는 라벨도 없는데도 학습한다니, 기계보다 훨씬 정교하다.
> 하지만 언젠가 기계가 '왜 학습하는가'를 이해하게 된다면,
> 우리도 인간이나 오리처럼 '라벨 없이도' 배우는 방법을 더 깊이 이해할 수 있을 것이다.”

📌 이 책의 첫 장에서 저자는 '간단한 회귀 문제 해결법'을 통해 딥러닝의 출발점이 되는 핵심 개념들 — 선형 관계, 지도학습, 가중치 학습 — 을 자연스럽게 소개하고, 이것이 결국 복잡한 심층 신경망으로 이어지는 첫걸음이라는 점을 강조하는 거죠.

_어떠신가요? 너무 재밌지 않나요.. 이 책은 이렇게 작게 작게 뜯어서 읽어보면 너무 흥미로운 부분이 많아 하고싶은 말이 많아져 상중하편에 이 책 한 권을 다룰 수 있을지 의문이 들지만 같이 잘 읽어보아요 ㅎㅎ!_

## 최초의 인공 신경세포

> 워런 매컬러는 미국의 신경생리학자로, 1930년대에 신경해부학을 연구하면서 원숭이 뇌 부위의 연결지도를 작성했는데, 그러는 와중에 뇌의 논리에도 심취해 있었다. 그즈음 앨런 튜링, 앨프리드 노스 .. 같은 수학자와 철학자들의 연구는 연산과 논리 사이에 깊은 연광성이 있음을 암시했다. "p가 참이고 q가 참이면 s는 참이다" 같은 진술은 논리 명제의 예이다. 그들의 주장은 모든 연산을 이런 논리로 환원할 수 있다는 것이었다. 연산에 대해 이런 사고방식을 가진 매컬러는 다음과 같은 의문으로 골머리를 썩였다. 많은 사람들이 생각하듯이 뇌가 연산 장치라면 뇌는 이런 논리를 어떻게 구현하는 것일까? -p21

### 1️⃣ "뇌의 논리"란 뭘까?

"뇌의 논리"는 뇌가 생각하거나 결정을 내리는 방식에 어떤 논리적 규칙이 있지 않을까? 하는 질문이에요.

예를 들어 사람이 어떤 사실 두 개를 보고(입력) 하나의 결론을 내릴 때(출력),

그 과정이 논리적인 구조를 따르는 건 아닐까? 느낌이죠.
(예: "비가 온다" → "우산을 쓴다" 같은)

📌 뇌의 논리 = 인간의 사고가 논리 규칙을 따른다면, 그 논리 구조를 뇌는 어떻게 구현할까?

> "논리 명제"란
> **논리 명제(logical proposition)**는 참(True) 또는 거짓(False)으로 명확히 판단할 수 있는 문장이에요.

```
p: 비가 온다 → 참
q: 우산을 들고 나간다 → 참
s: 나는 젖지 않는다 → 참
```

p ∧ q ⇒ s
즉, "p가 참이고 q가 참이면 s도 참이다" 같은 식이 논리 명제예요. 이건 논리학의 기초 규칙이라 합니다.
그렇기에 "모든 연산을 이런 논리로 환원할 수 있다"는 건 컴퓨터의 가장 근본적인 사고방식이에요.

<덧셈, 뺄셈, 곱셈, 비교, 조건 분기...>
사실 모두를 아주 기초적인 논리 연산 (AND, OR, NOT) 으로 바꿀 수 있어요.

앨런 튜링이나 그 당시 수학자들은 **"모든 계산은 논리로 표현될 수 있다"**는 사실을 밝혔고,
매컬러는 이 생각을 보고 "그렇다면 뇌도 논리를 수행하는 구조일 수 있지 않을까?" 라고 생각한 거죠.

### 2️⃣ 뇌가 연산 장치라면, 뇌는 이런 논리를 어떻게 구현하는 걸까?

여기서 핵심 질문이 시작돼요. 굉장히 흥미로운 부분이였어요.

💭 “논리 회로는 전기적으로 구현할 수 있어. 그런데 뇌는 전기 신호로 작동하잖아?
그럼 뇌도 논리 명제를 구현하는 장치처럼 볼 수 있지 않을까?”

그래서 매컬러는 이렇게 생각해요. 뉴런들이 어떻게 연결되어 있고, 어떻게 발화(활성화)되는지를 살펴보면, 마치 AND / OR / NOT 논리 회로처럼 작동할 수도 있겠다!

그리고 그 생각을 수학적으로 처음 표현한 사람이 바로 매컬러 & 피츠였고,
그 논문이 바로 퍼셉트론의 뿌리, 즉 인공 신경망의 탄생이라는 걸 알 수 있었어요.

### 3️⃣ 매컬러와 피츠는 이 모형을 단순한 계산 모형인 인공 신경세포로 구현했다!

![](https://velog.velcdn.com/images/imeureka/post/b95ab931-127e-4569-b19d-25b4f1ad5224/image.png)

**"인공 신경세포"** 라는 개념을 처음 소개하고 있어요. 이건 진짜 뇌에 있는 신경세포를 흉내 낸 단순한 계산 기계라고 생각하면 돼요.

사람의 뇌에는 수많은 신경세포(뉴런)가 서로 연결되어 정보를 전달하고 반응해요. 여기까지는 잘 이해가 되실거라 생각합니다!

이런 방식에서 아이디어를 얻은 두 학자, 매컬러(McCulloch)와 피츠(Pitts)는 실제 뉴런처럼 작동하는 매우 단순한 모델을 만들었고, 이것을 **"인공 신경세포"(책에서는 "뉴로드"라고도 부름)**라고 불렀어요.

### 4️⃣ 단일 뉴로드란,

"뉴로드"는 뇌의 뉴런(neuron) + 분기점 노드(node)의 합성어로, 입력들을 받아서 계산한 후 결과를 출력하는 가장 기본적인 유닛이에요.

**즉, 하나의 뉴런처럼 작동하는 계산 단위!**

이 모델은 실제 뇌처럼 복잡하지 않고, 0과 1을 다루는 간단한 계산 기능만 하는 구조예요. AND, OR, NOT 같은 기본적인 논리 연산을 할 수 있어요. 즉, 특정 입력이 들어오면 그걸 계산해서 맞으면 1, 틀리면 0을 출력하는 방식이에요. 이런 방식은 디지털 회로나 컴퓨터의 논리 연산과 아주 유사해요.

즉, 이 구조는 이렇게 작동해요 (이걸 수학적으로 표현한 게 MCP 모델)

```
1. 입력값들을 받는다 (예: x1, x2)
2. 이들을 더한다
3. 어떤 기준(임곗값, θ)보다 크거나 같으면 1을 출력, 아니면 0을 출력
```

앞서 책의 그림은 이런 인공 신경세포가 어떻게 생겼고 어떻게 작동하는지를 단순하게 나타낸 거예요.
입력이 두 개(x1, x2) 들어오고, 그걸 더해서 일정 기준(임계값) 이상이면 결과를 1로 출력하고, 아니면 0으로 출력해요.

여기까지 이해했다면 이걸 이제 논리 게이트를 예로 보면

> MCP 모형에서 입력값은 이진값 (보통 0 또는 1)

`AND 게이트`는 둘 다 1일 때만 1이 돼야 하죠.
그럼 x1 + x2 = 2일 때만 1이 나오면 되니까
👉 θ를 2로 설정!

`OR 게이트`는 둘 중 하나만 1이어도 1이 돼야 해.
그럼 x1 + x2 = 1 이상이면 1이 나오면 되니까
👉 θ를 1로 설정!

> 이 수식들이 의미하는 바는?
> ![](https://velog.velcdn.com/images/imeureka/post/4d1246a1-929d-4d8a-87eb-63a7e0c4031e/image.png)

여기서 g(x)는 모든 입력값(x1, x2, ..., xn)을 더한 값이에요. 그리고 f는 함수인데, z를 받아서 θ와 비교하죠. z가 기준값보다 작으면 0, 크거나 같으면 1을 내보낸다는 의미의 두번째 함수식을 풀어쓴거고, g(x)로 입력값들을 더한 다음, f(g(x))로 θ와 비교해서 0 또는 1을 출력합니다.

이게 바로 인공 신경세포 하나가 **"입력 → 판단 → 출력"**하는 전체 흐름이에요!

📌 그래서 결론, 인공 신경세포는 여러 개의 입력값을 더한 뒤, 그 합이 어떤 기준값(세타, θ)보다 크거나 같으면 1을 출력하고, 그렇지 않으면 0을 출력해요.
이 과정을 수학적으로는 g(x)라는 함수를 통해 입력값을 모두 더하고, 그 결과값을 f() 함수로 판단해서 출력값 y를 결정해요.
이런 구조만 가지고도 AND, OR 같은 기본 논리 회로를 만들 수 있어요.
예를 들어 두 입력값이 모두 1일 때만 1을 출력하는 AND 논리는, 기준값 θ를 2로 설정하면 만들 수 있어요.

> **※ 참고 : "임계값"과 "세타(θ)"란,**
> 이건 말 그대로 기준값이에요.
> 신호(0 또는 1)가 여러 개 들어오면, 그걸 더해서 합이 이 기준보다 크거나 같아야 결과가 1이 되는 거죠.
> 예를 들어

```
x1 = 1, x2 = 1 (총합은 2)
θ = 2 (임계값이 2)
```

이럴 땐 총합 2가 임계값 2보다 크거나 같으니까 결과 y는 1이 되는 것이고
반대로

```
x1 = 1, x2 = 0 (총합은 1)
θ = 2
```

이건 합이 기준보다 작기 때문에, 결과 y는 0이 되는 거죠.
즉, "어느 정도 강한 자극이 들어왔을 때만 반응해!" 라는 뇌의 작동 방식과 똑같은 거랍니다..

### 5️⃣ 입력은 '억제성inhibitory'으로 만들 수 있다.

> MCP 모형은 단순하지만 확장할 수 있다. 입력 개수를 늘리면 된다. 입력은 억제성으로 만들 수 있는데, 이것은 x1 이나 x2 에 -1 을 곱할 수 있다는 뜻이다. 뉴로드 입력 중 하나가 억제성이고 임곗값이 그에 맞게 정해지면, 뉴로드는 나머지 모든 입력값과 무관하게 언제나 0을 출력한다. 이 방법을 쓰면 더 복잡한 논리를 구성할 수 있다. 여러 개의 뉴로드를 연결하면 한 뉴로드의 출력이 다른 뉴로드에 입력되로록 할 수 있다. -p25

보통 뉴런은 입력값을 다 더하고, 임계값(θ)을 넘으면 1, 아니면 0을 출력하죠.
하지만 입력값 중 하나에 `-1`을 곱하면, 그 입력은 전체 합에서 감점처럼 작용하게 됩니다.

```
입력 x1 = 1, x2 = 1, x3 = 1
가중치 w1 = 1, w2 = 1, w3 = -1
합: g(x) = 1×1 + 1×1 + 1×(-1) = 1 + 1 - 1 = 1
```

→ x3가 감점 역할을 했죠. 이런 입력을 **"억제성 입력"**이라고 부르는 거죠.
즉, 억제성 입력 하나만으로도 출력을 완전히 차단할 수 있는 상황을 만들 수 있다는 뜻으로 확장 가능합니다.

더불어 이 억제성 개념을 활용하면, `NOT 게이트`처럼 특정 입력이 있을 때 출력을 꺼버리는 회로를 만들 수 있습니다.

### 6️⃣ MCP 모형의 한계 - “그럼에도 결론은 학습할 수 있는 기계가 아니라 단순히 계산할 수 있는 기계였다.”

MCP 모형은 `계산`만 가능하고 `학습`은 못한다는 말입니다.

예를 들어, AND, OR 같은 논리 연산은 미리 정해진 가중치와 임계값(θ)만 있으면 계산할 수 있습니다.
하지만 입력 데이터가 바뀌거나 더 복잡한 문제(예: 손글씨 인식 등)를 풀려면, 스스로 가중치와 임계값을 바꿔가며 학습해야 하죠. MCP는 그게 불가능했습니다.

> 즉, 오늘날 신경망은 데이터를 보면서 스스로 학습합니다.
> → 예: 손글씨 데이터를 보며, 어떤 θ (혹은 bias)가 적절한지를 자동으로 학습함.
> 하지만 MCP는 그런 기능이 없었기에 이런 한계가 있었습니다.
> → 데이터를 보고 스스로 기준값(θ)을 조정하는 능력이 없었음.

📍 **퍼셉트론**이 등장하면서 이 한계가 해결됐고, 가중치와 임계값을 자동으로 학습하게 됐습니다..
그래서 퍼셉트론이 MCP보다 훨씬 강력하고 "진짜 인공 지능"의 출발점이 되었죠!

## 실수로부터 배우기

> 매컬러와 피츠가 발전시킨 신경세포 모형의 인공 신경세포 연결망은 학습능력이 없었다. 반면에 헤브는 생물학적 신경세포의 맥락에서 학습 메커니즘을 제안했는데, 그것을 간결하지만 다소 오해의 소지가 있게 표현했다. "함께 발화하는 신경세포는 하나로 연결된다" 더 정확히 말해보겠다.이 사고방식에 따르면 우리 뇌가 학습하는 이유는 한 신경세포의 출력이 다른 신경세포의 발화와 일관되게 연관될 때는 신경세포 사이의 연결이 강해지고 그렇지 않을 때는 약해지기 때문이다. 이 과정을 헤브 학습Hebbian learning이라고 부른다.로젠 블랫은 이 선구자들의 업적을 받아들여 새로운 발상으로 종합했다. 그의 인공 신경세포는 재구성이라는 방법으로 학습하며 정보를 연결의 세기로 구체화한다. -p28

### 1️⃣ “함께 발화하는 뉴런은 하나로 연결된다”는 무슨 뜻?

이전에 말했듯이 MCP(매컬러와 피츠가 발전시킨)모델의 한계점은 θ(임계값)과 가중치(입력별 영향도)를 기계가 스스로 못 찾는다는 점 기억하시죠?
즉, 사람이 “이 조건이면 θ는 2, 이건 -1 곱하자” 이렇게 손으로 넣어줘야 했죠. 하지만 도널드 헤브의 사고방식이자 이 아이디어를 정리한 걸 **헤브 학습(Hebbian Learning)** 는

`발화한다` = 신경세포가 전기 신호를 보냄 (1 출력)
`함께 발화한다` = 두 신경세포가 동시에 1 출력을 자주 함

두 뉴런이 자주 같이 1을 출력하면, 그 연결이 더 강해집니다.

> 즉, `A 뉴런`이 1을 자주 낼 때 `B 뉴런`도 1을 자주 낸다면 A → B 사이의 연결이 강해진다.
> 반대로,`A 뉴런`는 1인데 `B 뉴런`는 자주 0이라면 A → B 사이의 연결은 약해진다.
> **이걸 반복하면 신경망은 스스로 어떤 연결을 강화하고 약화하며 ‘학습’하게 됩니다.**

이를 기반으로 프랭크 로젠블랫은 MCP와 헤브 이론을 바탕으로 **“가중치를 스스로 조정해서 학습하는 퍼셉트론”**을 만들었어요.

> 퍼셉트론 = "컴퓨터가 가중치를 스스로 업데이트" 할 수 있도록 설계된 최초의 학습 가능한 인공 신경망
> 퍼셉트론 1호는 이미지를 보고서 알파벳을 인식할 수 있었다. ... "요점은 퍼셉트론 1호가 실수를 저지를 때마다 혼나는 방식으로 글자를 인식하는 법을 배웠다는 것이라고요!" 너지는 여러 번 이렇게 말했다.

퍼셉트론은 실수로부터 배워 가중치와 편향을 조정합니다.

> 퍼셉트론 장치의 제작은 대단한 성취였다. 하지만 훨씬 큰 성취는 만일 데이터가 선형적으로 분리 가능하면 단층 퍼셉트론이 선형 분리 초평면을 반드시 찾아낸다는 수학 증명이었다. 이 증명을 이해하려면 벡터가 무엇이며 어떻게 이것들이 기계 학습에서 데이터를 나타내는 방법의 뼈대를 이루는지 알아야한다. 이것이 우리의 첫 번째 수학적 급유 지점이다.

### 2️⃣ 훨씬 큰 성취는… 단층 퍼셉트론이 선형 분리 초평면을 반드시 찾아낸다는 수학 증명이었다.

“데이터가 선형적으로 나눠질 수 있다면, 단층 퍼셉트론은 그 경계선을 반드시 찾아낸다”는 게 수학적으로 증명되었다는 말이죠.

> 참고 : 선형 분리란?
> 쉽게 말하면, 데이터를 한 줄(혹은 초평면)로 정확히 나눌 수 있는 경우랍니다.

```
 o     o   ← 클래스 1
       |
 x     x   ← 클래스 2
```

여기서는 세로 선 하나로 o와 x를 나눌 수 있죠? 이게 선형 분리 가능이라는 뜻입니다.

퍼셉트론이 데이터를 어떻게 계산하냐면, 데이터를 벡터(숫자의 나열) 로 보고, 그 벡터가 어느 방향(공간상의 위치)에 있는지를 보고 분류합니다.

### 3️⃣ 퍼셉트론의 한계

퍼셉트론은 “선형적으로만” 데이터를 나눌 수 있습니다.

예시: XOR 문제
XOR 데이터는 다음과 같습니다 :

```
(0,0) → 0
(1,0) → 1
(0,1) → 1
(1,1) → 0
```

그래프에 찍으면, 어느 선을 그어도 두 클래스를 직선 하나로 나눌 수 없습니다.

그래서 XOR 같은 문제는 단층 퍼셉트론으로 해결 불가능합니다.
→ 이게 퍼셉트론의 치명적인 한계였고, 이걸 지적한 사람이 바로 마빈 민스키 (AI 겨울의 원인 중 하나).

## 🚀 1장 마무리하며

이렇게 1장을 통해 우리는 머신러닝, 특히 딥러닝의 출발점이 되는 선형 모델과 지도학습, 그리고 가중치 학습의 개념을 만나봤어요.ㅎㅎ 5시간? 걸린 거 같아요..

단순해 보이는 y = w1x1 + w2x2 같은 수식 안에, 기계가 데이터를 통해 세상의 규칙을 배워가는 아주 근본적인 원리가 담겨 있었죠.

이 과정에서 제가 생각하는 가장 중요한 건 ‘패턴을 찾는 일’이에요.(그러니 이 1장 이름도 패턴을 찾고 말 테다 겠죠?) 데이터를 보고 숨겨진 규칙을 발견하고, 그 규칙을 이용해 새로운 상황을 예측하는 것. 기계학습의 핵심은 그저 복잡한 기술이 아니라, 세상의 어떤 것도 데이터와 패턴으로 연결 지어볼 수 있다는 생각의 확장입니다.

저자는 이를 새끼 오리와 비교하며,(이게 진짜 머리 띵하게 만드는 인상적인 부분이였던 거 같아요)
기계가 더 정교한 학습을 하기 위해선 인간처럼 라벨 없이도 배우는 방법에 다가가야 한다고 말해요.

결국 이 단순한 선형 모델의 이야기는, 인공지능이 ‘학습’이라는 개념을 어떻게 시작하고, 어디로 나아갈 수 있는지를 보여주는 첫걸음인 셈이에요! 2장도 잘 정리해서 와보겠습니다
